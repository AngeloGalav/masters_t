intro here

\section{Preliminary observation of the results}
Before examining the results based on In this section, we will present and comment on some examples of text generated by the 

Before testing, a select group of configuration undergo LoRA fine-tuning using these two task-specific datasets. Then, the results were compared between baseline compressed models and their LoRA-enhanced counterparts isolate the effectiveness of parameter-efficient fine-tuning in recovering compression-induced degradation.

Additionally, cross-task evaluation examines the transferability of LoRA adaptations by testing TriviaQA-tuned models on WikiText-2 perplexity and WikiText-2-tuned models on TriviaQA accuracy. This cross-evaluation reveals the task-specificity of LoRA adaptations and their potential for broader performance improvements beyond their target domains.
\section{Results on TriviaQA}
\section{Results on WikiText}