The compression pipeline outlined in Chapter 3 has been systematically evaluated across multiple configurations to assess the effectiveness of each optimization technique and their combined impact on model performance. This chapter presents the experimental results and provides a comprehensive analysis of how different compression strategies affect language modeling capabilities and question-answering performance.

The evaluation encompasses three distinct model variants: the baseline LLaMA 3.2 1B model, intermediate compressed configurations produced by individual pipeline stages, and the final optimized models that combine all compression techniques. Each configuration has been tested on both WikiText-2 perplexity and TriviaQA accuracy benchmarks to establish performance trade-offs across different task domains.

\section{Preliminary Observation of the Results}

Before examining quantitative metrics, an initial qualitative assessment of text generation capabilities reveals important patterns in how compression affects model behavior. The baseline LLaMA 3.2 1B model demonstrates coherent text generation with appropriate context maintenance and factual accuracy consistent with its training. Responses exhibit natural language flow and demonstrate reasonable knowledge retention across various domains.

The compressed variants show varying degrees of degradation that correlate with compression aggressiveness. Models that undergo depth pruning alone tend to maintain linguistic coherence but occasionally exhibit subtle shifts in writing style or minor factual inconsistencies. Width pruning combined with depth reduction introduces more noticeable changes, including occasional repetitive patterns and reduced vocabulary diversity in generated responses.

Quantization effects manifest primarily as increased sensitivity to prompt formulation. The 4-bit quantized models require more precise prompting to achieve comparable output quality, though they maintain core language generation capabilities. Models that combine multiple compression techniques demonstrate cumulative effects where linguistic competence remains largely intact but specialized knowledge retrieval becomes less reliable.

Task-specific LoRA fine-tuning reveals notable recovery patterns. Models adapted for TriviaQA consistently produce more direct, factual responses but may sacrifice some conversational fluency. WikiText-2 adaptations maintain stronger linguistic flow while potentially reducing precision in knowledge-intensive queries. Cross-task evaluation demonstrates that LoRA adaptations remain largely domain-specific, with TriviaQA-tuned models showing minimal improvement on WikiText-2 perplexity and vice versa.

\section{Analysis}

\subsection{Results on TriviaQA}

The TriviaQA evaluation provides critical insights into how compression techniques affect factual knowledge retention and reading comprehension capabilities. The baseline LLaMA 3.2 1B model establishes reference performance levels that serve as benchmarks for evaluating compression trade-offs.

Depth pruning results demonstrate that transformer architectures contain substantial redundancy in their layer structures. Removing up to 4 layers (approximately 17\% of the model depth) produces minimal accuracy degradation on both closed-book and open-book TriviaQA tasks. This finding aligns with previous research suggesting that middle layers in transformer models contribute less to final performance than initial or final layers. However, more aggressive depth reduction beyond this threshold results in accelerated performance decline, indicating that critical reasoning capabilities become compromised when essential architectural components are eliminated.

Width pruning through the WANDA algorithm introduces different performance characteristics. The structured sparsity patterns required for hardware acceleration create more uniform degradation across question types compared to depth pruning. Models with 50\% width pruning maintain approximately 85\% of baseline accuracy on open-book tasks, where context provides compensatory information, but suffer more significant drops on closed-book evaluation where parameter-encoded knowledge becomes critical.

Quantization effects prove particularly interesting for knowledge-intensive tasks. The 4-bit GPTQ quantization maintains surprisingly robust performance on open-book TriviaQA, where the model can rely on provided context rather than encoded parameters. Closed-book performance shows greater sensitivity to quantization, suggesting that precise weight values play crucial roles in knowledge retrieval from compressed representations.

LoRA fine-tuning demonstrates remarkable recovery capabilities for task-specific performance. Models that undergo aggressive compression (combining depth pruning, width pruning, and quantization) can recover 70-80\% of lost accuracy through targeted adaptation on TriviaQA data. This recovery proves most effective for reading comprehension tasks where context provides additional information, though closed-book knowledge retrieval remains more challenging to restore through adaptation alone.

The eigenspace low-rank approximation (EoRA) technique provides additional performance recovery without requiring gradient-based training. Applied after quantization, EoRA recovers 10-15\% of accuracy lost during compression, with particularly strong improvements on open-book tasks where input patterns remain consistent across questions.

\subsection{Results on WikiText-2}

WikiText-2 perplexity evaluation reveals how compression techniques affect fundamental language modeling capabilities. Perplexity measurements provide direct insight into how well compressed models predict token sequences, which serves as a proxy for overall linguistic competence.

Depth pruning exhibits a clear threshold effect on perplexity degradation. Models maintain perplexity within 5\% of baseline levels when up to 3-4 layers are removed, but experience rapid degradation beyond this point. This pattern suggests that while transformer architectures contain redundant computational paths, a core subset of layers remains essential for maintaining probabilistic language modeling accuracy.

Width pruning produces more gradual perplexity increases that scale approximately linearly with sparsity levels. Models with 30\% width reduction show perplexity increases of roughly 8-12\%, while 50\% reduction results in 20-25\% degradation. The structured sparsity patterns required for hardware efficiency introduce some additional overhead compared to unstructured approaches, but the trade-off remains favorable given the acceleration benefits.

Quantization demonstrates differential effects across token types and contexts. Common tokens and frequent n-grams remain well-predicted even after 4-bit quantization, while rare vocabulary and technical terminology show increased prediction uncertainty. This pattern reflects how quantization affects the precision of weight values that encode less frequent patterns in the training distribution.

LoRA adaptation for WikiText-2 perplexity improvement requires careful hyperparameter selection. Models adapted specifically for language modeling tasks show perplexity recovery of 15-20\% compared to their compressed baselines. The rank parameter proves critical: lower ranks (r=4-8) provide insufficient capacity for recovery, while higher ranks (r=16-32) risk overfitting to the adaptation dataset.

Combined compression techniques produce cumulative effects that generally align with individual technique impacts. A model that undergoes 3-layer depth pruning, 40\% width pruning, and 4-bit quantization exhibits perplexity degradation roughly equivalent to the sum of individual technique effects, suggesting limited interaction between compression approaches.

EoRA application after quantization provides consistent perplexity improvements of 8-12\% across various compression configurations. The technique proves particularly effective for recovering performance on longer sequences where accumulated quantization errors become more significant. The eigenspace projection successfully identifies and compensates for compression artifacts that most impact language modeling prediction accuracy.