% Chapter 5: Conclusions

The compression pipeline developed throughout this thesis represents a comprehensive approach to reducing the computational and memory requirements of large language models while maintaining acceptable performance levels. By combining multiple optimization techniques in a carefully orchestrated sequence, the work demonstrates that aggressive model compression remains viable even for already-distilled architectures like LLaMA 3.2 1B.

The sequential application of depth pruning, width pruning, LoRA adaptation, quantization, and EoRA compensation creates a systematic pathway from the original 1B parameter model to significantly more compact variants suitable for deployment on resource-constrained hardware.

The evaluation framework reveals that these techniques, when properly integrated, can achieve substantial compression ratios while preserving core language modeling capabilities. The WikiText-2 perplexity results demonstrate that compressed models retain their fundamental ability to predict token sequences, while TriviaQA evaluations confirm that both factual knowledge retention and reading comprehension abilities survive the compression process with manageable degradation for moderate compression ratios, and can often be restored using a LoRA adapter in configurations where heavy performance loss occurs.

However, the work also highlights several limitations that constrain its immediate applicability. The most significant challenge lies in the substantial deterioration observed under aggressive compression ratios, where even refinement techniques such as LoRA adaptation and EoRA compensation cannot fully recover the capabilities lost during extensive pruning, resulting in models that struggle to maintain coherent language generation and factual accuracy for the most memory-constrained edge devices. Additionally, the sequential nature of the pipeline may not capture optimal interactions between different compression stages. The evaluation framework also presents limitations, as assessments concentrated primarily on English language tasks and, despite using relatively large prompts for reading comprehension tasks, focused on relatively short-context scenarios compared to the extended sequences that modern LLMs are increasingly expected to handle, leaving uncertainty about compression behavior on longer sequences and multilingual applications.

Nevertheless, this work establishes a solid foundation for practical model compression and demonstrates that multi-stage optimization approaches leveraging pruning techniques and quantization represent a viable pathway toward developing both environmentally sustainable LLMs and making advanced AI capabilities accessible across low-powered, resource-constrained devices.

\section{Future Work} \label{future_work}

While the models produced by the compression pipeline developed in this thesis demonstrate promising results across multiple evaluation benchmarks, numerous avenues remain unexplored that could significantly enhance both the quality of compressed models and the scope of the framework itself. These directions encompass methodological refinements, algorithmic innovations, and broader architectural compatibility considerations that could substantially improve the project's capabilities and applicability.

\subsection{Knowledge Distillation Integration}

One of the most significant limitations of the current approach lies in its reliance on pre-distilled models rather than incorporating distillation directly into the compression pipeline. As outlined in Section \ref{distillation_paragraph}, knowledge distillation represents a rather effective compression technique, yet computational constraints prevented its integration into this work. Future research should prioritize developing distillation workflows that can operate within reasonable computational budgets.

In this context, an interesting direction could involve adopting approaches such as \textit{Homotopic Distillation} (HomoDistil) \cite{homodistil}, which combines iterative pruning with knowledge distillation in a unified framework. HomoDistil (Section \ref{distillation_paragraph}) addresses the capacity gap problem by starting with the full teacher model and gradually removing neurons while simultaneously distilling knowledge, maintaining small prediction discrepancies throughout the process. This approach could be particularly well-suited to the current pipeline, as it directly addresses the performance degradation observed under aggressive pruning by providing continuous guidance during the compression process rather than attempting recovery afterward.

\subsection{Additional Quantization Methodologies}

While GPTQ proved effective in the current pipeline, the quantization landscape continues evolving rapidly with new algorithms which address different aspects of the precision-accuracy trade-off. \textit{Activation-aware Weight Quantization} (AWQ) \cite{awq} represents a particularly promising alternative that identifies salient weight channels based on activation distributions rather than weight magnitudes alone. AWQ's key insight that protecting only 1\% of the most important weights can dramatically reduce quantization error suggests potential for even more aggressive compression than achieved with GPTQ. Unlike GPTQ's reliance on Hessian information, AWQ employs mathematically equivalent transformations to scale salient channels, avoiding hardware-inefficient mixed-precision schemes while maintaining better generalization across domains.

\textit{Quality Quattuor-bit Quantization} (QQQ) \cite{qqq} offers another compelling approach that addresses the performance-speed trade-off through W4A8 quantization (4-bit weights, 8-bit activations). The method combines two key innovations: adaptive smoothing that selectively targets activation channels with significant outliers while preserving other channels, and Hessian-based compensation that employs the same mathematical framework as GPTQ for iterative weight adjustments to minimize quantization losses. This dual approach enables QQQ to handle both weight and activation quantization simultaneously, potentially achieving better compression ratios than weight-only methods while maintaining acceptable results. Notably, QQQ is natively supported by the GPTQModel toolkit \cite{gptqmodel}, making integration into the existing pipeline straightforward and potentially offering superior performance recovery compared to current weight-only quantization approaches.

\subsection{Advanced Investigations of Interdependencies} \label{advanced_interdep}

The current pipeline implements compression techniques sequentially with minimal consideration of their interactions, yet understanding how different stages influence each other could provide insights into significantly improved compression strategies. Future investigations should systematically examine adaptive methodologies that adjust later techniques based on earlier results, following a similar approach to 2SSP \cite{2ssp} (Section \ref{pruning_paragraph_chap2}); for instance, the ratios and thresholds for width pruning could be dynamically determined based on the specific layers removed during depth pruning, potentially targeting different sparsity levels in regions where architectural changes have already occurred.

Likewise, applying consistent methodologies across compression stages warrants exploration, such as using perplexity-based importance for both depth and width pruning rather than the current mixed approach of perplexity for layers and magnitude-based metrics for weights. The layer preservation employed in depth pruning, which safeguards the first four and last two layers from removal, could also be extended to width pruning operations. This would protect critical architectural components across both compression dimensions while enabling more aggressive optimization in intermediate regions.

The unification of recovery methods represents another possible improvement. Rather than applying LoRA and EoRA as separate sequential steps, these techniques could potentially be unified into a single adapter mechanism, in order to simplify the recovery process by eliminating intermediate steps.

Otherwise, EoRA can be leveraged as superior starting point for LoRA fine-tuning. The original EoRA paper \cite{eora} demonstrates that using EoRA matrices to initialize LoRA substantially enhances accuracy recovery compared to standard methods. This strategy could prove particularly effective for heavily compressed models, where EoRA's eigenspace projection provides a more informed foundation for subsequent gradient-based optimization.

Alternative sequencing configurations also warrant systematic investigation. Rather than the current Depth$\rightarrow$Width$\rightarrow$LoRA progression, arrangements such as Depth$\rightarrow$LoRA$\rightarrow$Width could allow for performance recovery immediately after the most aggressive structural changes, potentially preserving more model capabilities during subsequent fine-grained pruning\footnote{Preliminary experiments on this Depth$\rightarrow$LoRA$\rightarrow$Width configuration were conducted, with results presented in Appendix \ref{app:appendix2}.}.

\subsection{Extended Model Compatibility}
The current pipeline's focus on LLaMA architectures, while strategic for this work, limits its broader applicability. Extending support to popular open-weight model families like DeepSeek \cite{deepseek} and Qwen \cite{qwen} would significantly increase the framework's utility and enable broader comparative studies across different architectural paradigms.

While some components of the pipeline already support these architectures through underlying libraries like GPTQModel, which provides native support for both DeepSeek and Qwen models \cite{gptqmodel}, other aspects of the framework may not accommodate these changes seamlessly. The compression pipeline's layer importance calculations, pruning strategies, and evaluation protocols were specifically designed around LLaMA's architectural characteristics and may require substantial modifications for different model families. Additionally, availability constraints affect the practicality of extending to certain architectures: DeepSeek models are not available in small language model configurations comparable to LLaMA 3.2 1B, limiting their suitability for the target deployment scenarios. Conversely, Qwen offers promising alternatives with 1B and even 0.5B parameter variants in the Qwen2 series \cite{qwen2}, potentially enabling exploration of even more aggressive compression ratios than achievable with LLaMA.

Beyond expanding to existing architectures, the pipeline design should anticipate future model developments. Creating modular, architecture-agnostic compression components that can be easily adapted to new Transformer variants would ensure the framework's longevity.

Finally, one must consider that different architectures may exhibit varying sensitivities to compression techniques, requiring architecture-specific tuning of pruning criteria, quantization parameters, and LoRA configurations. Systematic studies of how compression techniques interact with different architectural choices could inform more effective optimization strategies and reveal whether the current pipeline's assumptions hold across diverse model families.

\subsection{Evaluation Framework Improvements}

The current evaluation methodology provides solid insights into compression effects on language model performance, yet several expansions can applied to increase the scope and depth of future assessments.

Extending the linguistic scope presents an opportunity to explore compression behavior across diverse language families. Different idioms exhibit varying degrees of morphological complexity that could interact differently with the pipeline, particularly when comparing agglutinative systems to isolating ones. Large-scale multilingual datasets like HPLT \cite{hplt}, covering 75 distinct tongues, offer promising avenues for comprehensive cross-linguistic evaluation and could reveal language-specific  vulnerabilities or robustness patterns. Additionally, it would be interesting to experiment with how LoRA adaptation on different languages from HPLT would influence performance on the WikiText-2 and TriviaQA evaluation methods previously adopted in this work.

Incorporating longer context evaluation represents another valuable enhancement. The focus on relatively short-context scenarios fails to capture compression behavior on larger sequences that increasingly define modern LLM applications. Thus, understanding how compression affects performance on sequences spanning thousands of tokens becomes essential. Extended context evaluation would be particularly valuable for compressed models, as the interaction between reduced parameter counts and long-range dependencies may create failure modes not fully visible in shorter evaluation scenarios.

The diversity of task assessments could be broadened. The current focus on perplexity and reading comprehension does not address the broad spectrum of capabilities that modern LLMs possess. Code generation represents a particularly valuable extension, as programming tasks require precise logical reasoning and structured output generation that may be disproportionately affected by parameter reduction. Recent proposals like CodeJudge \cite{codejudge} show that LLM-based evaluation can assess semantic correctness of generated code without requiring traditional test cases, and thus could be incorporated more easily into the evaluation framework.

\subsection{KV Cache Compression}
Key-value cache compression represents a critical bottleneck for auto-regressive generation that the current pipeline does not address. During inference, the KV cache stores attention keys and values for all previous tokens to prevent re-computation, but its size grows linearly with sequence length, creating substantial memory pressure that often exceeds hardware limitations \cite{kvcompr}.

Numerous techniques exist for KV cache compression, including quantization methods that reduce the precision of stored key-value pairs and pruning approaches that selectively remove components based on their significance \cite{kvcompr2}. Similar to how the current pipeline applies importance-based pruning to model parameters, KV cache optimizations can exploit fine-grained differences in significance across multiple dimensions (e.g. the differing computational impact of keys versus values in attention mechanisms, or the varying importance of individual tokens based on their contribution to subsequent predictions). Rather than applying uniform compression to all cache components, these approaches can selectively preserve the most critical elements while aggressively compressing or removing less important ones.

Recent frameworks like LeanKV \cite{kvcompr2} demonstrate that such differentiated approaches can achieve substantial compression ratios while maintaining near-lossless accuracy on complex reasoning tasks. Integrating similar KV cache optimization techniques into the pipeline could further optimize memory consumption beyond the current parameter reduction focus, and result in a more comprehensive approach that addresses both static model size and dynamic inference memory requirements.

\subsection{Engineering Improvements} \label{sec:future_work_engineering}
Several implementation enhancements could improve the practical performance of the compression pipeline without requiring algorithmic innovations. Flash Attention \cite{flash_attention} and its variants represent a direct optimization opportunity, as these techniques reorganize attention computation to minimize memory transfers between GPU memory hierarchies while maintaining mathematical equivalence to standard attention. Since Flash Attention operates independently of model compression, integrating it into the inference workflow would complement the existing compression techniques by reducing runtime memory pressure during the pipeline execution and model assessment phases without affecting model weights or architecture. This could be implemented through Hugging Face's BetterTransformer \cite{bettertransformer}, which provides optimized attention kernels including Flash Attention for supported architectures. However, BetterTransformer compatibility remains limited to specific model families, potentially creating challenges when extending the pipeline to Transformer variants beyond LLaMA 2 and 3, where such optimizations may not be readily available.

Supporting hardware-accelerated structured sparsity represents another feature that could significantly improve the pipeline's evaluation performance for models pruned in a structured fashion. As described in Section \ref{wanda}, NVIDIA's Ampere and Hopper GPU architectures provide native support for structured sparse matrix operations that can achieve theoretical 2$\times$ speedups over dense computations \cite{nvidia-width}. While the current pipeline produces models with structured sparsity patterns, the inference implementation does not exploit these hardware capabilities. Adapting the framework to leverage these acceleration features would require substantial compatibility efforts, as structured sparsity support remains in PyTorch's nightly builds rather than stable releases at the time of writing \cite{pytorch_sparsity}. In addition, implementation would likely necessitate redefining the Transformer architecture using the TorchAO optimization library \cite{torchao}, which provides the necessary primitives for structured sparse operations but requires careful integration with existing inference workflows.

Both improvements focus on optimizing the development and evaluation workflow rather than advancing compression methodologies themselves. While they could substantially enhance the practical utility and efficiency of the pipeline's operation, they do not necessarily translate to other embedded architectures where the models may be deployed, instead requiring specific hardware-dependent implementations that may not be available across all target platforms.