% Chapter 5: Conclusions

The compression pipeline developed throughout this thesis represents a comprehensive approach to reducing the computational and memory requirements of large language models while maintaining acceptable performance levels. By combining multiple optimization techniques in a carefully orchestrated sequence, the work demonstrates that aggressive model compression remains viable even for already-distilled architectures like LLaMA 3.2 1B.

The sequential application of depth pruning, width pruning, LoRA adaptation, quantization, and EoRA compensation creates a systematic pathway from the original 1B parameter model to significantly more compact variants suitable for deployment on resource-constrained hardware. Each stage addresses different aspects of model efficiency: depth pruning eliminates architectural redundancy at the macro level, width pruning refines parameter importance at the micro level, LoRA recovers lost performance while enabling task adaptation, quantization dramatically reduces memory footprint, and EoRA provides training-free accuracy recovery for quantized models.

The evaluation framework reveals that these techniques, when properly integrated, can achieve substantial compression ratios while preserving core language modeling capabilities. The WikiText-2 perplexity results demonstrate that compressed models retain their fundamental ability to predict token sequences, while TriviaQA evaluations confirm that both factual knowledge retention and reading comprehension abilities survive the compression process with manageable degradation for moderate compression ratios, and can often be restored using a LoRA adapter in configurations where degradation occurs.

However, the work also highlights several limitations that constrain its immediate applicability. The most significant challenge lies in the substantial degradation observed under aggressive compression ratios, where even the application of refinement techniques such as LoRA adaptation and EoRA compensation cannot fully recover the capabilities lost during extensive pruning. While moderate compression levels maintain acceptable performance, pushing toward the extreme compression ratios required for the most memory-constrained edge devices results in models that struggle to maintain coherent language generation and factual accuracy. Additionally, the sequential nature of the pipeline may not capture optimal interactions between different compression stages. The evaluation framework also presents limitations, as assessments concentrated primarily on English language tasks and, despite using relatively large prompts for reading comprehension tasks, focused on relatively short-context scenarios compared to the extended sequences that modern LLMs are increasingly expected to handle, leaving uncertainty about compression behavior on longer sequences and multilingual applications.

Nevertheless, this work establishes a solid foundation for practical model compression and demonstrates that multi-stage optimization approaches leveraging pruning techniques and quantization represent a viable pathway toward developing both environmentally sustainable LLMs and making advanced AI capabilities accessible across low-powered, resource-constrained devices.

\section{Future Work} \label{future_work}

While the models produced by the compression pipeline developed in this thesis demonstrate promising results across multiple evaluation benchmarks, numerous avenues remain unexplored that could significantly enhance both the quality of compressed models and the scope of the framework itself. These directions encompass methodological refinements, algorithmic innovations, and broader architectural compatibility considerations that could substantially improve the project's capabilities and applicability.

\subsection{Knowledge Distillation Integration}

One of the most significant limitations of the current approach lies in its reliance on pre-distilled models rather than incorporating distillation directly into the compression pipeline. As outlined in Section \ref{distillation_paragraph}, knowledge distillation represents a rather effective compression technique, yet computational constraints prevented its integration into this work. Future research should prioritize developing distillation workflows that can operate within reasonable computational budgets.

In this context, an interesting direction could involve adopting approaches such as \textit{Homotopic Distillation} (HomoDistil) \cite{homodistil}, which combines iterative pruning with knowledge distillation in a unified framework. HomoDistil (Section \ref{distillation_paragraph}) addresses the capacity gap problem by starting with the full teacher model and gradually removing neurons while simultaneously distilling knowledge, maintaining small prediction discrepancies throughout the process. This approach could be particularly well-suited to the current pipeline, as it directly addresses the performance degradation observed under aggressive pruning by providing continuous guidance during the compression process rather than attempting recovery afterward.

\subsection{Additional Quantization Methodologies}

While GPTQ proved effective in the current pipeline, the quantization landscape continues evolving rapidly with new algorithms addressing different aspects of the precision-accuracy trade-off. \textit{Activation-aware Weight Quantization} (AWQ) \cite{awq} represents a particularly promising alternative that identifies salient weight channels based on activation distributions rather than weight magnitudes alone. AWQ's key insight that protecting only 1\% of the most important weights can dramatically reduce quantization error suggests potential for even more aggressive compression than achieved with GPTQ. Unlike GPTQ's reliance on Hessian information, AWQ employs mathematically equivalent transformations to scale salient channels, avoiding hardware-inefficient mixed-precision schemes while maintaining better generalization across domains without overfitting to calibration data.

\textit{Quality Quattuor-bit Quantization} (QQQ) \cite{qqq} offers another compelling approach that addresses the performance-speed trade-off through W4A8 quantization (4-bit weights, 8-bit activations). The method combines two key innovations: adaptive smoothing that selectively targets activation channels with significant outliers while preserving other channels, and Hessian-based compensation that employs the same mathematical framework as GPTQ for iterative weight adjustments to minimize quantization losses. This dual approach enables QQQ to handle both weight and activation quantization simultaneously, potentially achieving better compression ratios than weight-only methods while maintaining acceptable results. Notably, QQQ is natively supported by the GPTQModel toolkit \cite{gptqmodel}, making integration into the existing pipeline straightforward and potentially offering superior performance recovery compared to current weight-only quantization approaches.

\subsection{Advanced Investigations of Interdependencies}

The current pipeline implements compression techniques sequentially with minimal consideration of their interactions, yet understanding how different stages influence each other could provide insights into significantly improved compression strategies. Future investigations should systematically examine adaptive methodologies that adjust later techniques based on earlier results; for instance, the ratios and thresholds for width pruning could be dynamically determined based on the specific layers removed during depth pruning, potentially targeting different sparsity levels in regions where architectural changes have already occurred.

Similarly, applying consistent methodologies across compression stages warrants exploration, such as using perplexity-based importance for both depth and width pruning rather than the current mixed approach of perplexity for layers and magnitude-based metrics for weights. The layer preservation employed in depth pruning, which safeguards the first four and last two layers from removal, could also be extended to width pruning operations. This would protect critical architectural components across both compression dimensions while enabling more aggressive optimization in intermediate regions.

The unification of recovery methods represents another possible improvement. Rather than applying LoRA and EoRA as separate sequential steps, these techniques could potentially be unified into a single adapter mechanism. While current implementations treat these methods independently, developing integrated approaches could simplify the recovery process by eliminating intermediate steps and potentially improving computational efficiency during model adaptation.

Otherwise, EoRA can be leveraged as superior starting point for LoRA fine-tuning. The original EoRA paper \cite{eora} demonstrates that using EoRA matrices to initialize LoRA substantially enhances accuracy recovery compared to standard methods. This strategy proves particularly effective for heavily compressed models, where EoRA's eigenspace projection provides a more informed foundation for subsequent gradient-based optimization, potentially bridging the gap between training-free compensation and full fine-tuning approaches.

Alternative sequencing configurations also warrant systematic investigation. Rather than the current depth-width-LoRA progression, arrangements such as depth-LoRA-width could allow for performance recovery immediately after the most aggressive structural changes, potentially preserving more model capabilities during subsequent fine-grained pruning.

\subsection{Extended Model Compatibility}
The current pipeline's focus on LLaMA architectures, while strategic for this work, limits its broader applicability. Extending support to popular open-weight model families like DeepSeek \cite{deepseek} and Qwen \cite{qwen} would significantly increase the framework's utility and enable broader comparative studies across different architectural paradigms.

While some components of the pipeline already support these architectures through underlying libraries like GPTQModel, which provides native support for both DeepSeek and Qwen models \cite{gptqmodel}, other aspects of the framework may not accommodate these changes seamlessly. The compression pipeline's layer importance calculations, pruning strategies, and evaluation protocols were specifically designed around LLaMA's architectural characteristics and may require substantial modifications for different model families. Additionally, availability constraints affect the practicality of extending to certain architectures: DeepSeek models are not available in small language model configurations comparable to LLaMA 3.2 1B, limiting their suitability for the target deployment scenarios. Conversely, Qwen offers promising alternatives with 1B and even 0.5B parameter variants in the Qwen2 series \cite{qwen2}, potentially enabling exploration of even more aggressive compression ratios than achievable with LLaMA.

Beyond expanding to existing architectures, the pipeline design should anticipate future model developments. Creating modular, architecture-agnostic compression components that can be easily adapted to new transformer variants would ensure the framework's longevity.

Finally, one must consider that different architectures may exhibit varying sensitivities to compression techniques, requiring architecture-specific tuning of pruning criteria, quantization parameters, and LoRA configurations. Systematic studies of how compression techniques interact with different architectural choices could inform more effective optimization strategies and reveal whether the current pipeline's assumptions hold across diverse model families.

\subsection{KV Cache Compression}
Key-value cache compression represents a critical bottleneck for autoregressive generation that the current pipeline does not address. During transformer inference, the KV cache stores attention keys and values for all previous tokens to prevent re-computation, but its size grows linearly with sequence length, creating substantial memory pressure that often exceeds hardware limitations \cite{kvcompr}.

Numerous techniques exist for KV cache compression, including quantization methods that reduce the precision of stored key-value pairs and pruning approaches that selectively remove components based on their significance \cite{kvcompr2}. Similar to how the current pipeline applies importance-based pruning to model parameters, KV cache compression can exploit fine-grained differences in significance across multiple dimensions (e.g. the differing computational impact of keys versus values in attention mechanisms, or the varying importance of individual tokens based on their contribution to subsequent predictions). Rather than applying uniform compression to all cache components, these approaches can selectively preserve the most critical elements while aggressively compressing or removing less important ones.

Recent frameworks like LeanKV \cite{kvcompr2} demonstrate that such differentiated approaches can achieve substantial compression ratios while maintaining near-lossless accuracy on complex reasoning tasks. Integrating similar KV cache compression techniques into the pipeline could further optimize memory consumption beyond the current parameter reduction focus, and result in a more comprehensive approach that addresses both static model size and dynamic inference memory requirements.

\subsection{Engineering Improvements} \label{sec:future_work_engineering}
Several implementation enhancements could improve the practical performance of the compression pipeline without requiring algorithmic innovations. Flash Attention \cite{flash_attention} and its variants represent a direct optimization opportunity, as these techniques reorganize attention computation to minimize memory transfers between GPU memory hierarchies while maintaining mathematical equivalence to standard attention. Since Flash Attention operates independently of model compression, integrating it into the inference pipeline would complement the existing compression techniques by reducing runtime memory pressure without affecting model weights or architecture. This could be implemented through Hugging Face's BetterTransformer \cite{bettertransformer}, which provides optimized attention kernels including Flash Attention for supported architectures. However, BetterTransformer compatibility remains limited to specific model families, potentially creating challenges when extending the pipeline to transformer variants beyond LLaMA 2 and 3, where such optimizations may not be readily available.

Supporting hardware-accelerated structured sparsity represents another implementation enhancement that could significantly improve inference performance for models pruned in a structured fashion. As described in Section \ref{wanda}, NVIDIA's Ampere and Hopper GPU architectures provide native support for structured sparse matrix operations that can achieve theoretical 2x speedups over dense computations \cite{nvidia-width}. The current pipeline produces models with structured sparsity patterns, but the inference implementation does not exploit these hardware capabilities. Adapting the framework to leverage these acceleration features would require substantial compatibility efforts, as structured sparsity support remains in PyTorch's nightly builds rather than stable releases at the time of writing \cite{pytorch_sparsity}. In addition, implementation would likely necessitate redefining the transformer architecture using the TorchAO optimization library \cite{torchao}, which provides the necessary primitives for structured sparse operations but requires careful integration with existing model loading and inference workflows.

Both improvements focus on optimizing the deployment aspects of compressed models rather than advancing compression methodologies themselves, yet they could substantially enhance the practical utility of the pipeline's outputs.