% Chapter 2: Background and Related Work
Before explaining the details and implementation of the methodology used in this project, it is essential to provide an overview of the evolution of the inner workings of the Transformer architecture as well as Large Language Models. In addition, we will also discuss revelant compression techniques that have been developed in this context, and how they influenced this work.
Finally, we will also shed some light on the target hardware, whose limitations have been a driving force behind the design choices made in this project.

\section{The architecture of Transformers}

% [explain SLMs here]
\section{The structure of Large Language Models}

\section{Relevant compression techniques}


