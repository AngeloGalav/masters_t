\begin{thebibliography}{9}
	\addcontentsline{toc}{chapter}{References}

	\bibitem{attention_is_all_you_need}
	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin,
	\textit{Attention is All You Need},
	\url{https://arxiv.org/abs/1706.03762}.

	\bibitem{bert}
	Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova,
	\textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	\url{https://arxiv.org/abs/1810.04805}.

	\bibitem{gpt}
	Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
	\textit{Improving Language Understanding by Generative Pre-Training},
	\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}.

	\bibitem{gpt4}
	OpenAI,
	\textit{GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses},
	\url{https://openai.com/index/gpt-4/}.

	\bibitem{academic_integrity}
	Mike Perkins,
	\textit{Academic Integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond},
	\url{https://www.researchgate.net/publication/368775737_Academic_integrity_considerations_of_AI_Large_Language_Models_in_the_post-pandemic_era_ChatGPT_and_beyond}.

	\bibitem{artistic_integrity}
	Daniel Mügge,
	\textit{AI Is Threatening More Than Just Creative Jobs—It’s Undermining Our Humanity},
	\url{https://www.socialeurope.eu/ai-is-threatening-more-than-just-creative-jobs-its-undermining-our-humanity}.


	\bibitem{target_hardware}
	Arpan Suravi Prasad, Moritz Scherer, Francesco Conti, Davide Rossi, Alfio Di Mauro, Manuel Eggimann, Jorge Tómas Gómez, Ziyun Li, Syed Shakib Sarwar, Zhao Wang, Barbara De Salvo, Luca Benini
	\textit{Siracusa: A 16 nm Heterogenous RISC-V SoC for Extended Reality with At-MRAM Neural Engine},
	\url{https://arxiv.org/abs/2312.14750}.

	\bibitem{rnn}
	Robin M. Schmidt,
	\textit{Recurrent Neural Networks (RNNs): A gentle Introduction and Overview},
	\url{https://arxiv.org/pdf/1912.05911}

	\bibitem{rnn_details}
	Chris Nicholson,
	\textit{A Beginner’s Guide to LSTMs and Recurrent Neural Networks}.
	\url{ https://skymind.ai/wiki/lstm}.

	\bibitem{pytorch}
	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, et al.,
	\textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	\url{https://arxiv.org/abs/1912.01703}.

	\bibitem{franken-llama}
	Angelo Galavotti,
	\textit{FRANKEN-LLAMA code repository},
	\url{https://github.com/AngeloGalav/franken-llama}

	\bibitem{hellaswag}
	Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi
	\textit{HellaSwag: Can a Machine Really Finish Your Sentence?}
	\url{https://arxiv.org/abs/1905.07830}.

	\bibitem{llama}
	Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, et al.,
	\textit{LLaMA: Open and Efficient Foundation Language Models},
	\url{https://arxiv.org/abs/2302.13971}.

	\bibitem{llama2}
	Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, et al.,
	\textit{Llama 2: Open Foundation and Fine-Tuned Chat Models},
	\url{https://arxiv.org/abs/2307.09288}.

	\bibitem{llama3}
	Llama Team, AI @ Meta,
	\textit{The Llama 3 Herd of Models},
	\url{https://arxiv.org/abs/2407.21783}.

	\bibitem{general_llm_pruning}
	Hanjuan Huang, Hao-Jia Song, Hsing-Kuo Pao,
	\textit{Large Language Model Pruning},
	\url{https://arxiv.org/abs/2406.00030}.

	\bibitem{shortened_llama}
	Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song,
	\textit{Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods},
	\url{https://arxiv.org/abs/2402.02834}.

	\bibitem{sheared_llama}
	Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen
	\textit{Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning},
	\url{https://arxiv.org/abs/2310.06694}.

	\bibitem{gptqmodel}
	ModelCloud,
	\textit{GPTQModel},
	\url{https://github.com/ModelCloud/GPTQModel}.

	\bibitem{gptq_quantization}
	Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh
	\textit{GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
	\url{https://arxiv.org/abs/2210.17323}

	\bibitem{wanda}
	Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter,
	\textit{A Simple and Effective Pruning Approach for Large Language Models},
	\url{https://arxiv.org/abs/2306.11695}.

	\bibitem{eora}
	Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, et al.,
	\textit{EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation},
	\url{https://arxiv.org/abs/2410.21271}.

	\bibitem{lora}
	Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
	\textit{LoRA: Low-Rank Adaptation of Large Language Models},
	\url{https://arxiv.org/abs/2106.09685}.

	\bibitem{triviaqa}
	Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer,
	\textit{TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
	\url{https://arxiv.org/abs/1705.03551}.

	\bibitem{wikitext}
	Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher,
	\textit{Pointer Sentinel Mixture Models},
	\url{https://arxiv.org/abs/1609.07843}.

	\bibitem{c4}
	Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu,	
	\textit{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	\url{https://arxiv.org/pdf/1910.10683}.

	\bibitem{homodistil}
	Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, Tuo Zhao,
	\textit{HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers},
	\url{https://arxiv.org/abs/2302.09632}.

\end{thebibliography}