\begin{thebibliography}{9}
	\addcontentsline{toc}{chapter}{References}

	\bibitem{academic_integrity}
	Mike Perkins,
	\textit{Academic Integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond},
	\url{https://www.researchgate.net/publication/368775737_Academic_integrity_considerations_of_AI_Large_Language_Models_in_the_post-pandemic_era_ChatGPT_and_beyond}

	\bibitem{artistic_integrity}
	Daniel Mügge,
	\textit{AI Is Threatening More Than Just Creative Jobs—It’s Undermining Our Humanity},
	\url{https://www.socialeurope.eu/ai-is-threatening-more-than-%
	just-creative-jobs-its-undermining-our-humanity}

	\bibitem{gpt_energy}
	David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, Jeff Dean,
	\textit{Carbon Emissions and Large Neural Network Training},
	\url{https://arxiv.org/abs/2104.10350}

	\bibitem{datacenter_energy}
	Thomas Spencer, Siddharth Singh,
	\textit{What the data centre and AI boom could mean for the energy sector},
	\url{https://www.iea.org/commentaries/what-the-data-centre-and-ai-boom-could-mean-for-the-energy-sector}

	\bibitem{hungry_ai}
	Nidhal Jegham, Marwen Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi,
	\textit{How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference}
	\url{https://arxiv.org/abs/2505.09598v1}

	\bibitem{google_report}
	Google,
	\textit{Google Environmental Report 2023},
	\url{https://sustainability.google/reports/google-2023-environmental-report-executive-summary/}

	\bibitem{ai_water}
	Pengfei Li, Jianyi Yang, Mohammad A. Islam, Shaolei Ren,
	\textit{Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models},
	\url{https://arxiv.org/abs/2304.03271}

	\bibitem{gpt}
	Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
	\textit{Improving Language Understanding by Generative Pre-Training},
	\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}

	\bibitem{gpt4}
	OpenAI,
	\textit{GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses},
	\url{https://openai.com/index/gpt-4/}

	\bibitem{rnn}
	Chris Nicholson,
	\textit{A Beginner’s Guide to LSTMs and Recurrent Neural Networks}
	\url{https://skymind.ai/wiki/lstm}

	\bibitem{lstm}
	S. Hochreiter, J. Schmidhuber,
	\textit{Long Short-Term Memory},
	\url{https://doi.org/10.1162/neco.1997.9.8.1735}

	\bibitem{hinton-lstm}
	Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton,
	\textit{Speech Recognition with Deep Recurrent Neural Networks},
	\url{https://arxiv.org/abs/1303.5778}

	\bibitem{lstm_textgeneration}
	Mustafa Abbas Hussein Hussein, Serkan Savaş,
	\textit{LSTM-Based Text Generation: A Study on Historical Datasets},
	\url{https://arxiv.org/abs/2403.07087}

	\bibitem{bpe}
	Philip Gage,
	\textit{A New Algorithm for Data Compression},
	\url{http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM}

	\bibitem{attention_is_all_you_need}
	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin,
	\textit{Attention is All You Need},
	\url{https://arxiv.org/abs/1706.03762}

	\bibitem{llama}
	Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, et al.,
	\textit{LLaMA: Open and Efficient Foundation Language Models},
	\url{https://arxiv.org/abs/2302.13971}

	\bibitem{llama2}
	Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, et al.,
	\textit{Llama 2: Open Foundation and Fine-Tuned Chat Models},
	\url{https://arxiv.org/abs/2307.09288}

	\bibitem{llama3}
	Llama Team, AI @ Meta,
	\textit{The Llama 3 Herd of Models},
	\url{https://arxiv.org/abs/2407.21783}

	\bibitem{target_hardware}
	Arpan Suravi Prasad, Moritz Scherer, Francesco Conti, Davide Rossi, Alfio Di Mauro, Manuel Eggimann, Jorge Tómas Gómez, Ziyun Li, Syed Shakib Sarwar, Zhao Wang, Barbara De Salvo, Luca Benini,
	\textit{Siracusa: A 16 nm Heterogenous RISC-V SoC for Extended Reality with At-MRAM Neural Engine},
	\url{https://arxiv.org/abs/2312.14750}
	
	\bibitem{distillation}
	Geoffrey Hinton, Oriol Vinyals, Jeff Dean,
	\textit{Distilling the Knowledge in a Neural Network},
	\url{https://arxiv.org/abs/1503.02531}
	
	\bibitem{homodistil}
	Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, Tuo Zhao,
	\textit{HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers},
	\url{https://arxiv.org/abs/2302.09632}

	\bibitem{2ssp}
	Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca,
	\textit{2SSP: A Two-Stage Framework for Structured Pruning of LLMs},
	\url{https://arxiv.org/abs/2501.17771}

	\bibitem{sheared_llama}
	Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen,
	\textit{Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning},
	\url{https://arxiv.org/abs/2310.06694}

	\bibitem{franken-llama}
	Angelo Galavotti,
	\textit{FRANKEN-LLAMA code repository},
	\url{https://github.com/AngeloGalav/franken-llama}

	\bibitem{pytorch}
	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, et al.,
	\textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	\url{https://arxiv.org/abs/1912.01703}

	\bibitem{hf_transformers}
	Hugging Face,
	\textit{Transformers library documentation},
	\url{https://huggingface.co/docs/transformers/index}

	\bibitem{hellaswag}
	Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi,
	\textit{HellaSwag: Can a Machine Really Finish Your Sentence?}
	\url{https://arxiv.org/abs/1905.07830}

	\bibitem{llama3_1b}
	Llama Team, AI @ Meta,
	\textit{Llama-3.2-1B},
	\url{https://huggingface.co/meta-llama/Llama-3.2-1B}

	\bibitem{llama3_1b_instruct}
	Llama Team, AI @ Meta,
	\textit{Llama-3.2-1B-Instruct},
	\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}

	\bibitem{shortened_llama}
	Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song,
	\textit{Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods},
	\url{https://arxiv.org/abs/2402.02834}

	\bibitem{wanda}
	Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter,
	\textit{A Simple and Effective Pruning Approach for Large Language Models},
	\url{https://arxiv.org/abs/2306.11695}

	\bibitem{nvidia-width}
	Hongxiao Bai, Yun Li,
	\textit{Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines},
	\url{https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/}

	\bibitem{lora}
	Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
	\textit{LoRA: Low-Rank Adaptation of Large Language Models},
	\url{https://arxiv.org/abs/2106.09685}

	\bibitem{peft}
	Hugging Face,
	\textit{PEFT: State-of-the-art Parameter-Efficient Fine-Tuning},
	\url{https://github.com/huggingface/peft}

	\bibitem{quant_cnn}
	Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng,
	\textit{Quantized Convolutional Neural Networks for Mobile Devices},
	\url{https://arxiv.org/abs/1512.06473}

	\bibitem{gptq_quantization}
	Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh,
	\textit{GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
	\url{https://arxiv.org/abs/2210.17323}

	\bibitem{obq}
	Elias Frantar, Sidak Pal Singh, Dan Alistarh,
	\textit{Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning},
	\url{https://arxiv.org/abs/2208.11580}

	\bibitem{gptqmodel}
	ModelCloud,
	\textit{GPTQModel},
	\url{https://github.com/ModelCloud/GPTQModel}

	\bibitem{c4}
	Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu,
	\textit{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	\url{https://arxiv.org/abs/1910.10683}

	\bibitem{eora}
	Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, et al.,
	\textit{EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation},
	\url{https://arxiv.org/abs/2410.21271}

	\bibitem{wikitext}
	Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher,
	\textit{Pointer Sentinel Mixture Models},
	\url{https://arxiv.org/abs/1609.07843}

	\bibitem{perplexity}
	F. Jelinek, R. L. Mercer, L. R. Bahl, J. K. Baker,
	\textit{Perplexity—a measure of the difficulty of speech recognition tasks},
	\url{https://doi.org/10.1121/1.2016299}

	\bibitem{triviaqa}
	Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer,
	\textit{TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
	\url{https://arxiv.org/abs/1705.03551}

	\bibitem{qqq}
	Ying Zhang, Peng Zhang, Mincong Huang, Jingyang Xiang, Yujie Wang, Chao Wang, Yineng Zhang, Lei Yu, Chuan Liu, Wei Lin,
	\textit{QQQ: Quality Quattuor-Bit Quantization for Large Language Models},
	\url{https://arxiv.org/abs/2406.09904}

	\bibitem{awq}
	Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han,
	\textit{AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
	\url{https://arxiv.org/abs/2306.00978}

	\bibitem{mistral}
	Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, et al.,
	\textit{Mistral 7B},
	\url{https://arxiv.org/abs/2310.06825}

	\bibitem{deepseek}
	DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, et al.,
	\textit{DeepSeek-V3 Technical Report},
	\url{https://arxiv.org/abs/2412.19437}

	\bibitem{qwen}
	Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, et al.,
	\textit{Qwen Technical Report},
	\url{https://arxiv.org/abs/2309.16609}

	\bibitem{qwen2}
	HuggingFace,
	\textit{Qwen's Collections},
	\url{https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f}

	\bibitem{kvcompr}
	Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, et al.,
	\textit{KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches},
	\url{https://arxiv.org/abs/2407.01527}

	\bibitem{kvcompr2}
	Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C.S. Lui, Haibo Chen,
	\textit{Unifying KV Cache Compression for Large Language Models with LeanKV},
	\url{https://arxiv.org/abs/2412.03131v2}

	\bibitem{flash_attention}
	Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré,
	\textit{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}
	\url{https://arxiv.org/abs/2205.14135}

	\bibitem{bettertransformer}
	HuggingFace,
	\textit{BetterTransformer Overview},
	\url{https://huggingface.co/docs/optimum/bettertransformer/overview}

	\bibitem{pytorch_sparsity}
	PyTorch,
	\textit{Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity},
	\url{https://pytorch.org/blog/accelerating-neural-network-training/}

	\bibitem{torchao}
	PyTorch,
	\textit{ao: PyTorch native quantization and sparsity for training and inference},
	\url{https://github.com/pytorch/ao}


\end{thebibliography}