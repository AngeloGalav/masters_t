\begin{thebibliography}{9}
	\addcontentsline{toc}{chapter}{References}

	\bibitem{academic_integrity}
	Mike Perkins,
	\textit{Academic Integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond},
	2023. DOI: 10.53761/1.20.02.07. URL: \url{https://doi.org/10.53761/1.20.02.07}

	\bibitem{artistic_integrity}
	Daniel Mügge,
	\textit{AI Is Threatening More Than Just Creative Jobs—It's Undermining Our Humanity},
	Social Europe, 2024. URL: \url{https://www.socialeurope.eu/ai-is-threatening-more-than-just-creative-jobs-its-undermining-our-humanity}

	\bibitem{gpt_energy}
	David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, Jeff Dean,
	\textit{Carbon Emissions and Large Neural Network Training},
	2021. arXiv: 2104.10350 [cs.LG]. URL: \url{https://arxiv.org/abs/2104.10350}

	\bibitem{datacenter_energy}
	Thomas Spencer, Siddharth Singh,
	\textit{What the data centre and AI boom could mean for the energy sector},
	IEA, 2024. URL: \url{https://www.iea.org/commentaries/what-the-data-centre-and-ai-boom-could-mean-for-the-energy-sector}

	\bibitem{hungry_ai}
	Nidhal Jegham, Marwen Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi,
	\textit{How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference},
	2025. arXiv: 2505.09598v1 [cs.CY]. URL: \url{https://arxiv.org/abs/2505.09598v1}

	\bibitem{google_report}
	Google,
	\textit{Google Environmental Report 2023}, 2023. URL:
	\url{https://sustainability.google/reports/google-2023-environmental-report-executive-summary/}

	\bibitem{ai_water}
	Pengfei Li, Jianyi Yang, Mohammad A. Islam, Shaolei Ren,
	\textit{Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models},
	2023. arXiv: 2304.03271 [cs.LG]. URL: \url{https://arxiv.org/abs/2304.03271}

	\bibitem{gpt}
	Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
	\textit{Improving Language Understanding by Generative Pre-Training}, OpenAI, 2018. URL:
	\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}

	\bibitem{gpt4}
	OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al.,
	\textit{GPT-4 Technical Report},
	2023. arXiv: 2303.08774 [cs.CL]. URL: \url{https://arxiv.org/abs/2303.08774}

	\bibitem{rnn}
	Razvan Pascanu, Tomas Mikolov, Yoshua Bengio,
	\textit{On the difficulty of training Recurrent Neural Networks},
	2012. arXiv:1211.5063 [cs.LG]. URL: \url{https://arxiv.org/abs/1211.5063}

	\bibitem{lstm}
	S. Hochreiter, J. Schmidhuber,
	\textit{Long Short-Term Memory}, 1997. DOI: 10.1162/neco.1997.9.8.1735.
	URL: \url{https://doi.org/10.1162/neco.1997.9.8.1735}

	\bibitem{hinton-lstm}
	Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton,
	\textit{Speech Recognition with Deep Recurrent Neural Networks},
	2013. arXiv: 1303.5778 [cs.NE]. URL: \url{https://arxiv.org/abs/1303.5778}

	\bibitem{lstm_textgeneration}
	Mustafa Abbas Hussein Hussein, Serkan Savaş,
	\textit{LSTM-Based Text Generation: A Study on Historical Datasets},
	2024. arXiv: 2403.07087 [cs.CL]. URL: \url{https://arxiv.org/abs/2403.07087}

	\bibitem{bpe}
	Philip Gage,
	\textit{A new algorithm for data compression}, 1994. DOI: 10.5555/177910.177914.
	URL: \url{https://dl.acm.org/doi/10.5555/177910.177914}

	\bibitem{attention_is_all_you_need}
	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin,
	\textit{Attention is All You Need},
	2017. arXiv: 1706.03762 [cs.CL]. URL: \url{https://arxiv.org/abs/1706.03762}

	\bibitem{llama}
	Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, et al.,
	\textit{LLaMA: Open and Efficient Foundation Language Models},
	2023. arXiv: 2302.13971 [cs.CL]. URL: \url{https://arxiv.org/abs/2302.13971}

	\bibitem{llama2}
	Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, et al.,
	\textit{Llama 2: Open Foundation and Fine-Tuned Chat Models},
	2023. arXiv: 2307.09288 [cs.CL]. URL: \url{https://arxiv.org/abs/2307.09288}

	\bibitem{llama3}
	Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, et al.,
	\textit{The Llama 3 Herd of Models},
	2024. arXiv: 2407.21783 [cs.AI]. URL: \url{https://arxiv.org/abs/2407.21783}

	\bibitem{target_hardware}
	Arpan Suravi Prasad, Moritz Scherer, Francesco Conti, Davide Rossi, Alfio Di Mauro, Manuel Eggimann, Jorge Tómas Gómez, Ziyun Li, Syed Shakib Sarwar, Zhao Wang, Barbara De Salvo, Luca Benini,
	\textit{Siracusa: A 16 nm Heterogenous RISC-V SoC for Extended Reality with At-MRAM Neural Engine},
	2023. arXiv: 2312.14750 [cs.AR]. URL: \url{https://arxiv.org/abs/2312.14750}
	
	\bibitem{distillation}
	Geoffrey Hinton, Oriol Vinyals, Jeff Dean,
	\textit{Distilling the Knowledge in a Neural Network},
	2015. arXiv: 1503.02531 [stat.ML]. URL: \url{https://arxiv.org/abs/1503.02531}
	
	\bibitem{homodistil}
	Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, Tuo Zhao,
	\textit{HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers},
	2023. arXiv: 2302.09632 [cs.CL]. URL: \url{https://arxiv.org/abs/2302.09632}

	\bibitem{2ssp}
	Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca,
	\textit{2SSP: A Two-Stage Framework for Structured Pruning of LLMs},
	2025. arXiv: 2501.17771 [cs.CL]. URL: \url{https://arxiv.org/abs/2501.17771}

	\bibitem{sheared_llama}
	Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen,
	\textit{Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning},
	2023. arXiv: 2310.06694 [cs.CL]. URL: \url{https://arxiv.org/abs/2310.06694}

	\bibitem{franken-llama}
	Angelo Galavotti,
	\textit{FRANKEN-LLAMA}, GitHub Repository, Last accessed July 2025. URL:
	\url{https://github.com/AngeloGalav/franken-llama}

	\bibitem{pytorch}
	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, et al.,
	\textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	2019. arXiv: 1912.01703 [cs.LG]. URL: \url{https://arxiv.org/abs/1912.01703}

	\bibitem{hf_transformers}
	Hugging Face,
	\textit{Transformers}, Library Documentation, Last accessed July 2025. URL:
	\url{https://huggingface.co/docs/transformers/index}

	\bibitem{hellaswag}
	Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi,
	\textit{HellaSwag: Can a Machine Really Finish Your Sentence?},
	2019. arXiv: 1905.07830 [cs.CL]. URL: \url{https://arxiv.org/abs/1905.07830}

	\bibitem{llama3_1b}
	Llama Team, AI @ Meta,
	\textit{Llama-3.2-1B},
	Hugging Face Model, 2024. URL: \url{https://huggingface.co/meta-llama/Llama-3.2-1B}

	\bibitem{llama3_1b_instruct}
	Llama Team, AI @ Meta,
	\textit{Llama-3.2-1B-Instruct}, Hugging Face Model, 2024. URL:
	\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}

	\bibitem{shortened_llama}
	Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song,
	\textit{Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods},
	2024. arXiv: 2402.02834 [cs.LG]. URL: \url{https://arxiv.org/abs/2402.02834}

	\bibitem{wanda}
	Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter,
	\textit{A Simple and Effective Pruning Approach for Large Language Models},
	2023. arXiv: 2306.11695 [cs.CL]. URL: \url{https://arxiv.org/abs/2306.11695}

	\bibitem{nvidia-width}
	Hongxiao Bai, Yun Li,
	\textit{Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines},
	NVIDIA Developer Blog, 2023. URL: \url{https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/}

	\bibitem{lora}
	Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
	\textit{LoRA: Low-Rank Adaptation of Large Language Models},
	2021. arXiv: 2106.09685 [cs.CL]. URL: \url{https://arxiv.org/abs/2106.09685}

	\bibitem{peft}
	Hugging Face,
	\textit{PEFT: State-of-the-art Parameter-Efficient Fine-Tuning},
	GitHub Repository, Last accessed July 2025. URL: \url{https://github.com/huggingface/peft}

	\bibitem{quant_cnn}
	Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng,
	\textit{Quantized Convolutional Neural Networks for Mobile Devices},
	2015. arXiv: 1512.06473 [cs.CV]. URL: \url{https://arxiv.org/abs/1512.06473}

	\bibitem{gptq_quantization}
	Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh,
	\textit{GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
	2022. arXiv: 2210.17323 [cs.LG]. URL: \url{https://arxiv.org/abs/2210.17323}

	\bibitem{obq}
	Elias Frantar, Sidak Pal Singh, Dan Alistarh,
	\textit{Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning},
	2022. arXiv: 2208.11580 [cs.LG]. URL: \url{https://arxiv.org/abs/2208.11580}

	\bibitem{gptqmodel}
	ModelCloud,
	\textit{GPTQModel},
	GitHub Repository, Last accessed July 2025. URL: \url{https://github.com/ModelCloud/GPTQModel}

	\bibitem{c4}
	Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu,
	\textit{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	2019. arXiv: 1910.10683 [cs.LG]. URL: \url{https://arxiv.org/abs/1910.10683}

	\bibitem{eora}
	Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, et al.,
	\textit{EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation},
	2024. arXiv: 2410.21271 [cs.CL]. URL: \url{https://arxiv.org/abs/2410.21271}

	\bibitem{wikitext}
	Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher,
	\textit{Pointer Sentinel Mixture Models},
	2016. arXiv: 1609.07843 [cs.CL]. URL: \url{https://arxiv.org/abs/1609.07843}

	\bibitem{perplexity}
	F. Jelinek, R. L. Mercer, L. R. Bahl, J. K. Baker,
	\textit{Perplexity—a measure of the difficulty of speech recognition tasks},
	1977. DOI: 10.1121/1.2016299. URL: \url{https://doi.org/10.1121/1.2016299}

	\bibitem{triviaqa}
	Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer,
	\textit{TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
	2017. arXiv: 1705.03551 [cs.CL]. URL: \url{https://arxiv.org/abs/1705.03551}

	\bibitem{qqq}
	Ying Zhang, Peng Zhang, Mincong Huang, Jingyang Xiang, Yujie Wang, Chao Wang, Yineng Zhang, Lei Yu, Chuan Liu, Wei Lin,
	\textit{QQQ: Quality Quattuor-Bit Quantization for Large Language Models},
	2024. arXiv: 2406.09904 [cs.LG]. URL: \url{https://arxiv.org/abs/2406.09904}

	\bibitem{awq}
	Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han,
	\textit{AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
	2023. arXiv: 2306.00978 [cs.CL]. URL: \url{https://arxiv.org/abs/2306.00978}

	\bibitem{deepseek}
	DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, et al.,
	\textit{DeepSeek-V3 Technical Report},
	2024. arXiv: 2412.19437 [cs.CL]. URL: \url{https://arxiv.org/abs/2412.19437}

	\bibitem{qwen}
	Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, et al.,
	\textit{Qwen Technical Report},
	2023. arXiv: 2309.16609 [cs.CL]. URL: \url{https://arxiv.org/abs/2309.16609}

	\bibitem{qwen2}
	Hugging Face,
	\textit{Qwen's Collections},
	Hugging Face Model Collection, Last accessed July 2025. URL: \url{https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f}

	\bibitem{kvcompr}
	Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, et al.,
	\textit{KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches},
	2024. arXiv: 2407.01527 [cs.CL]. URL: \url{https://arxiv.org/abs/2407.01527}

	\bibitem{kvcompr2}
	Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C.S. Lui, Haibo Chen,
	\textit{Unifying KV Cache Compression for Large Language Models with LeanKV},
	2024. arXiv: 2412.03131v2 [cs.LG]. URL: \url{https://arxiv.org/abs/2412.03131v2}

	\bibitem{flash_attention}
	Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré,
	\textit{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
	2022. arXiv: 2205.14135 [cs.LG]. URL: \url{https://arxiv.org/abs/2205.14135}

	\bibitem{bettertransformer}
	HuggingFace,
	\textit{BetterTransformer},
	Library Documentation, Last accessed July 2025. URL: \url{https://huggingface.co/docs/optimum/bettertransformer/overview}

	\bibitem{pytorch_sparsity}
	Jesse Cai, Daniel Haziza, Supriya Rao,
	\textit{Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity},
	PyTorch, 2024. URL: \url{https://pytorch.org/blog/accelerating-neural-network-training/}

	\bibitem{torchao}
	PyTorch,
	\textit{ao: PyTorch native quantization and sparsity for training and inference},
	GitHub Repository, Last accessed July 2025. URL: \url{https://github.com/pytorch/ao}

\end{thebibliography}